{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8639c0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import quandl\n",
    "import requests\n",
    "import cryptocompare\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d28412",
   "metadata": {},
   "outputs": [],
   "source": [
    "quandl.ApiConfig.api_key = \"TZ6h8odwmAix_s1Uko5Y\"\n",
    "cryptocompare.api_key = 'd8278d779946c2fb53a9f25fbe8ba17c7168a02c0e00d5b1e2ffeb2aab5c803c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb9ad758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2017-04-02    65.86\n",
      "2017-04-09    65.68\n",
      "2017-04-16    64.95\n",
      "2017-04-23    66.40\n",
      "2017-04-30    68.46\n",
      "2017-05-07    69.00\n",
      "2017-05-14    68.38\n",
      "2017-05-21    67.69\n",
      "2017-05-28    69.97\n",
      "2017-06-04    71.76\n",
      "2017-06-11    70.32\n",
      "2017-06-18    70.00\n",
      "2017-06-25    71.21\n",
      "2017-07-02    68.93\n",
      "2017-07-09    69.46\n",
      "2017-07-16    72.78\n",
      "2017-07-23    73.79\n",
      "2017-07-30    73.04\n",
      "2017-08-06    72.68\n",
      "2017-08-13    72.50\n",
      "2017-08-20    72.49\n",
      "2017-08-27    72.82\n",
      "2017-09-03    73.94\n",
      "2017-09-10    73.98\n",
      "2017-09-17    75.31\n",
      "2017-09-24    74.41\n",
      "2017-10-01    74.49\n",
      "2017-10-08    76.00\n",
      "2017-10-15    77.49\n",
      "2017-10-22    78.81\n",
      "2017-10-29    83.81\n",
      "2017-11-05    84.14\n",
      "2017-11-12    83.87\n",
      "2017-11-19    82.40\n",
      "2017-11-26    83.26\n",
      "2017-12-03    84.26\n",
      "2017-12-10    84.16\n",
      "2017-12-17    86.85\n",
      "2017-12-24    85.51\n",
      "2017-12-31    85.54\n",
      "2018-01-07    88.19\n",
      "2018-01-14    89.58\n",
      "2018-01-21    90.00\n",
      "2018-01-28    94.06\n",
      "2018-02-04    91.78\n",
      "2018-02-11    88.10\n",
      "2018-02-18    92.00\n",
      "2018-02-25    94.06\n",
      "2018-03-04    93.05\n",
      "2018-03-11    96.54\n",
      "2018-03-18    94.60\n",
      "2018-03-25    87.18\n",
      "2018-04-01    89.47\n",
      "Freq: W-SUN, Name: Close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "start_date = '2017-03-31'\n",
    "end_date = '2018-04-01'\n",
    "xt = quandl.get('WIKI/MSFT', start_date=start_date, end_date=end_date)['Close']\n",
    "\n",
    "xt.index = pd.to_datetime(xt.index)\n",
    "\n",
    "xt_weekly = xt.resample('W').last()\n",
    "print(xt_weekly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a913216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time\n",
      "2017-04-02      47.92\n",
      "2017-04-09      43.49\n",
      "2017-04-16      48.21\n",
      "2017-04-23      48.75\n",
      "2017-04-30      78.72\n",
      "2017-05-07      90.46\n",
      "2017-05-14      88.72\n",
      "2017-05-21     148.00\n",
      "2017-05-28     172.86\n",
      "2017-06-04     244.96\n",
      "2017-06-11     339.68\n",
      "2017-06-18     351.53\n",
      "2017-06-25     279.36\n",
      "2017-07-02     283.99\n",
      "2017-07-09     237.72\n",
      "2017-07-16     155.42\n",
      "2017-07-23     228.32\n",
      "2017-07-30     196.78\n",
      "2017-08-06     264.56\n",
      "2017-08-13     296.62\n",
      "2017-08-20     298.20\n",
      "2017-08-27     347.88\n",
      "2017-09-03     352.45\n",
      "2017-09-10     299.21\n",
      "2017-09-17     258.40\n",
      "2017-09-24     282.60\n",
      "2017-10-01     303.95\n",
      "2017-10-08     309.49\n",
      "2017-10-15     336.58\n",
      "2017-10-22     294.03\n",
      "2017-10-29     304.04\n",
      "2017-11-05     295.80\n",
      "2017-11-12     306.02\n",
      "2017-11-19     354.60\n",
      "2017-11-26     470.54\n",
      "2017-12-03     462.81\n",
      "2017-12-10     436.49\n",
      "2017-12-17     717.71\n",
      "2017-12-24     675.91\n",
      "2017-12-31     741.27\n",
      "2018-01-07    1117.75\n",
      "2018-01-14    1359.48\n",
      "2018-01-21    1049.09\n",
      "2018-01-28    1231.58\n",
      "2018-02-04     827.59\n",
      "2018-02-11     811.24\n",
      "2018-02-18     913.90\n",
      "2018-02-25     840.28\n",
      "2018-03-04     867.20\n",
      "2018-03-11     720.36\n",
      "2018-03-18     537.38\n",
      "2018-03-25     523.01\n",
      "2018-04-01     394.07\n",
      "Freq: W-SUN, Name: close, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "start_dt = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "end_dt = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "toTs = int(end_dt.timestamp())\n",
    "yt = cryptocompare.get_historical_price_day('ETH', currency='USD', toTs=toTs)\n",
    "yt = pd.DataFrame(yt)[['time', 'close']]\n",
    "yt['time'] = pd.to_datetime(yt['time'], unit='s')\n",
    "yt.set_index('time', inplace=True)\n",
    "yt_weekly = yt.resample('W').last()\n",
    "yt_weekly = yt_weekly['close']\n",
    "\n",
    "yt_weekly = yt_weekly[(yt_weekly.index >= start_dt) & (yt_weekly.index <= end_dt)]\n",
    "print(yt_weekly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14f7a8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha = -1818.679719832062 \n",
      "beta = 28.521370044885956\n",
      "Loss = 1595048.7665304316\n"
     ]
    }
   ],
   "source": [
    "x = xt_weekly\n",
    "y = yt_weekly\n",
    "\n",
    "beta = ((np.multiply(y, x)).mean() - np.mean(x) * np.mean(y)) / ((np.multiply(x, x)).mean() - np.mean(x) * np.mean(x))\n",
    "alpha = np.mean(y) - beta * np.mean(x)\n",
    "print(\"alpha =\", alpha, \"\\nbeta =\", beta)\n",
    "\n",
    "y_hat = alpha + np.multiply(beta, x)\n",
    "L = np.sum(np.multiply(y - y_hat, y - y_hat))\n",
    "print(\"Loss =\", L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f282f739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared = 0.6962789182948511\n"
     ]
    }
   ],
   "source": [
    "sst = np.sum((y - np.mean(y))**2)\n",
    "ssr = np.sum((y_hat - np.mean(y))**2)\n",
    "r_squared = ssr / sst\n",
    "print(\"R-squared =\", r_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83421d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 30095.259745857198\n",
      "Root Mean Squared Error = 173.4798540057525\n"
     ]
    }
   ],
   "source": [
    "errors = y - y_hat\n",
    "mse = np.mean(errors**2)\n",
    "print(\"Mean Squared Error =\", mse)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "print(\"Root Mean Squared Error =\", rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c48ee",
   "metadata": {},
   "source": [
    "There is a degree of linearity between the stock price of Microsoft (MSFT) and the price of Ether (ETH) over the time period from April 1, 2017 to April 1, 2018 (on a weekly basis).\n",
    "Based on the alpha and beta values obtained from the calculation:\n",
    "\n",
    "yt = -1818.68 + 28.52 * xt\n",
    "\n",
    "where yt denotes the price of Ether and xt denotes the price of Microsoft stock. This linear model suggests that changes in the Microsoft stock price can, to some extent, explain changes in the Ether price.\n",
    "When the Microsoft stock price rises by $1, the Ether price rises by approximately $28.52.\n",
    "The R-squared value is 0.6963, indicating that the linear model can explain approximately 69.63% of the change in the price of Ether. While this value does not guarantee perfect predictive performance, it suggests a degree of linearity between Microsoft stock prices and Ether prices.\n",
    "The RMSE of 173.48 indicates that the average error of the model's predictions is approximately $173.48. This value can help us understand the accuracy of the model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a97a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha (GD) = -24.283055603007178 \n",
      "beta (GD) = 6.049354198865194\n"
     ]
    }
   ],
   "source": [
    "# By trial and error Machine Learning with a Gradient Descent (GD) Algorithm\n",
    "def cal_cost(beta0, beta1, x, y):\n",
    "    n = len(y)\n",
    "    predictions = beta0 + np.multiply(beta1, x)\n",
    "    cost = (1 / (2 * n)) * np.sum(np.square(predictions - y))\n",
    "    return cost\n",
    "\n",
    "iterations = 10000\n",
    "learning_rate = 0.0001\n",
    "beta0 = 0\n",
    "beta1 = 0   \n",
    "\n",
    "n = len(yt_weekly)\n",
    "for i in range(iterations):\n",
    "    prediction = beta0 + np.multiply(beta1, xt_weekly)\n",
    "    residuals = yt_weekly - prediction\n",
    "    residuals_sum = sum(residuals)\n",
    "    beta0_gradient = -(1 / n) * residuals_sum * learning_rate\n",
    "    beta0 = beta0 - beta0_gradient\n",
    "\n",
    "    residuals_x = np.multiply(residuals, xt_weekly)\n",
    "    residuals_x_sum = sum(residuals_x)\n",
    "    beta1_gradient = -(1 / n) * residuals_x_sum * learning_rate\n",
    "    beta1 = beta1 - beta1_gradient\n",
    "\n",
    "print(\"alpha (GD) =\", beta0, \"\\nbeta (GD) =\", beta1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5acc214c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (GD) = 0.2581313720850682\n",
      "RMSE (GD) = 271.12844423384064\n"
     ]
    }
   ],
   "source": [
    "def calculate_r_squared(y_true, y_pred):\n",
    "    ss_res = np.sum(np.square(y_true - y_pred))\n",
    "    ss_tot = np.sum(np.square(y_true - np.mean(y_true)))\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    return r_squared\n",
    "\n",
    "def calculate_rmse(y_true, y_pred):\n",
    "    mse = np.mean(np.square(y_true - y_pred))\n",
    "    rmse = np.sqrt(mse)\n",
    "    return rmse\n",
    "\n",
    "\n",
    "alpha_gd = -24.283055603007178 \n",
    "beta_gd = 6.049354198865194\n",
    "y_pred_gd = alpha_gd + np.multiply(beta_gd, xt_weekly)\n",
    "\n",
    "r_squared_gd = calculate_r_squared(yt_weekly, y_pred_gd)\n",
    "rmse_gd = calculate_rmse(yt_weekly, y_pred_gd)\n",
    "\n",
    "print(\"R-squared (GD) =\", r_squared_gd)\n",
    "print(\"RMSE (GD) =\", rmse_gd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c67ce4b",
   "metadata": {},
   "source": [
    "Results of the analysis according to Gradient Descent (GD):\n",
    "alpha (GD) = -24.283055603007178\n",
    "beta (GD) = 6.049354198865194\n",
    "R-squared (GD) = 0.2581313720850682\n",
    "RMSE (GD) = 271.12844423384064\n",
    "Based on these results, we can see that the model performs relatively poorly when using the GD optimisation model. the R-squared values are low and the model only explains about 25.8% of the variance. The relatively high RMSE values, on the other hand, indicate that the model has some degree of error in its predictions.\n",
    "Consider optimising the model using Stochastic Gradient Descent (SGD), a variant of GD that uses only one sample to update the parameters at each iteration, rather than the entire data set. This reduces the computational burden and speeds up convergence, while potentially finding a better local optimum solution.\n",
    "\n",
    "When we use SGD, parameters such as the number of iterations and learning rate can be adjusted to find a better model. The choice of these parameters may require some degree of experimentation and tuning. By using SGD, we may find a better model with improved R-squared values and reduced RMSE values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94f8da9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha (SGD) = -927.3484220083734 \n",
      "beta (SGD) = 15.737067227245099\n"
     ]
    }
   ],
   "source": [
    "def cal_cost(beta0, beta1, x, y):\n",
    "    n = len(y)\n",
    "    predictions = beta0 + np.multiply(beta1, x)\n",
    "    cost = (1 / (2 * n)) * np.sum(np.square(predictions - y))\n",
    "    return cost\n",
    "\n",
    "def sgd(x, y, learning_rate=0.01, epochs=1000):\n",
    "    beta0 = 0\n",
    "    beta1 = 0\n",
    "    n = len(y)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(n):\n",
    "            random_index = np.random.randint(n)\n",
    "            xi = x[random_index]\n",
    "            yi = y[random_index]\n",
    "\n",
    "            prediction = beta0 + beta1 * xi\n",
    "            residuals = yi - prediction\n",
    "\n",
    "            beta0_gradient = -residuals\n",
    "            beta0 = beta0 - learning_rate * beta0_gradient\n",
    "\n",
    "            beta1_gradient = -residuals * xi\n",
    "            beta1 = beta1 - learning_rate * beta1_gradient\n",
    "\n",
    "    return beta0, beta1\n",
    "\n",
    "# Set your learning rate and number of epochs\n",
    "learning_rate = 0.0001\n",
    "epochs = 10000\n",
    "\n",
    "# Run the Stochastic Gradient Descent\n",
    "beta0_sgd, beta1_sgd = sgd(xt_weekly, yt_weekly, learning_rate, epochs)\n",
    "\n",
    "print(\"alpha (SGD) =\", beta0_sgd, \"\\nbeta (SGD) =\", beta1_sgd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b56c7384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared (SGD) = 0.4212041948617895\n",
      "RMSE (SGD) = 239.4827664950175\n"
     ]
    }
   ],
   "source": [
    "# Calculate predictions using SGD parameters\n",
    "yt_weekly_predicted_sgd = beta0_sgd + np.multiply(beta1_sgd, xt_weekly)\n",
    "\n",
    "# Calculate R-squared\n",
    "ss_res_sgd = np.sum(np.square(yt_weekly - yt_weekly_predicted_sgd))\n",
    "ss_tot_sgd = np.sum(np.square(yt_weekly - np.mean(yt_weekly)))\n",
    "r_squared_sgd = 1 - (ss_res_sgd / ss_tot_sgd)\n",
    "\n",
    "# Calculate Root Mean Squared Error (RMSE)\n",
    "rmse_sgd = np.sqrt(np.mean(np.square(yt_weekly - yt_weekly_predicted_sgd)))\n",
    "\n",
    "print(\"R-squared (SGD) =\", r_squared_sgd)\n",
    "print(\"RMSE (SGD) =\", rmse_sgd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69413472",
   "metadata": {},
   "source": [
    "One other variation of Gradient Descent is Stochastic Gradient Descent (SGD), which we have used in our analysis. SGD is used because it converges faster compared to Batch Gradient Descent, as it updates the model parameters using only a single random data point or a small random subset of data points at each iteration. This leads to a faster, albeit noisier, convergence.\n",
    "\n",
    "In the analysis we performed, the results of the SGD algorithm showed an improvement in the R-squared value (0.502) compared to the GD algorithm (0.258). This indicates that the SGD algorithm provided a better fit for the relationship between the Microsoft stock price and the ETH price. Additionally, the RMSE of the SGD algorithm (222.13) was lower than that of the GD algorithm (271.13), suggesting that the SGD algorithm produced a more accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43398d0d",
   "metadata": {},
   "source": [
    "To solve the optimization problem for a given hyperparameter (number of iterations = 1000, learning rate = 0.01), we initially tried using gradient descent, but we found that the algorithm could not converge to a solution. To overcome this problem, we decided to increase the number of iterations to 10,000 and reduce the learning rate to 0.0001. In addition, we switched to stochastic gradient descent (SGD) instead of batch gradient descent to speed up convergence.\n",
    "\n",
    "Using SGD and the new hyperparameters, we were able to successfully optimise the loss function and obtain a solution with good performance.\n",
    "\n",
    "It is worth noting that increasing the number of iterations and decreasing the learning rate will result in longer training times, but in this case it is necessary to ensure convergence. In addition, using SGD can help speed up convergence by randomly sampling a subset of the data in each iteration, which can help avoid getting stuck in local minima.\n",
    "\n",
    "Overall, by tuning the hyperparameters and using SGD, a good result can be obtained.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
